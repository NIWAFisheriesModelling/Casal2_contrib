---
title: "PosteriorPredictiveChecks"
author: "Casal2 Dev team"
date: "15/02/2022"
output: 
  html_vignette: default
  github_document: default
  bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{PosteriorPredictiveChecks}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
number_sections: false  
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction
This vignette demonstrates how to use Casal2s simulation mode with `r4Casal2` R functions to generate posterior predictive checks for goodness of fit measures for model fits. In terms of assessment workflow, this falls in the Diagnostic component.


The following vignette uses the Casal2 model embedded into this R package. If you want to see where this is on you system paste the following line of code into your R console `system.file("extdata", "PosteriorPredictiveChecks", package = "r4Casal2")`

![Assessment Process](Figures/assessment_process.png){width=50%}

```{r setup, warning=FALSE, echo=T, results='hide'}
# install Casal2 R-library
# install_github(https://github.com/NIWAFisheriesModelling/CASAL2/tree/master/R-libraries/casal2, ref = "HEAD")
library(Casal2, warn.conflicts = F)
library(r4Casal2, warn.conflicts = F)
library(tidyverse)
library(DHARMa)
```
 


## Estimation
Before looking at data goodness of fit you should be checking if the model has converged. We assume that the estimated model has satisfied this criteria i.e. invertable covariance, acceptable gradient (close to zero) and global minima (apposed to local try jittering start values).

```{r pressures, fig.width=6, fig.height=4}
mpd_file_name = system.file("extdata", "PosteriorPredictiveChecks","estimate.log", package = "r4Casal2", mustWork = TRUE)
mpd = extract.mpd(file = mpd_file_name)
# Report labels
names(mpd)
# is covariance symetric
isSymmetric(mpd$covar$covariance_matrix)
# is hessian invertable
is_matrix_invertable(mpd$hess$hessian_matrix)
```

## Simulations
The first thing you should do is add reports of type `simulated_observation` for each observation in your Casal2 configuration files. If you don't have these specified in your configuration files, Casal2 will not save simulated observations. Tips when specifying this report class

1. Save each simulated observation into a seperate `file_name`
2. Create a directory to save simulated data sets in.
3. Have the report label the same as the `file_name` (see example below)
4. Avoid haveing periods/dots (".") in `file_name`

An example report structure would look like
```{cmd report_example}
@report sim_chatTANage
type simulated_observation
observation chatTANage
file_name simulated_observations/sim_chatTANage
```

There are three variants of simulations you can conduct in Casal2, and these depend on if you are in MPD or MCMC estimation phase. If you are evaluating a MPD run, there are two variants and depend if you want to account for parameter uncertainty or not. If you don't want parameter uncertainty, then you need to run the following Casal2 command to produce 100 sets of simulations `casal2 -s 100 -i mpd_pars.log > simulate.log`. If you want to account for parameter uncertainty then you can use a multivariant normal distribution with mean equal to MPD and resulting covariance to produce a set of simulations, example below.
```{r sim_pars}
library(mvtnorm)
n_sims = 1000
## NOTE: might have issue with bounds assuming normal dist
sims = rmvnorm(n = n_sims, mean = as.numeric(mpd$estimate_value$values), sigma = mpd$covar$covariance_matrix)
dim(sims)
colnames(sims) = names(mpd$estimate_value$values)
## save simulated pars in the same directory as your
## CSL files
if(FALSE)
  write.table(sims, file = "mpd_mvnorm_pars.csl2", quote = F, row.names = F, col.names = T)
# run 
# casal2 -s 1 -i mpd_mvnorm_pars.csl2 > simulate.log
```

## Summarising simulated data in R
Assuming you have saved all the simulated observations as seperate files in a standalone folder.


```{r read_simulations}
sim_dir = system.file("extdata", "PosteriorPredictiveChecks","simulated_observations", package = "r4Casal2", mustWork = TRUE)

sim_vals = read.simulated.data(dir = sim_dir, mean_age = F)
# check no trouble with files
sim_vals$failed_files
# 
names(sim_vals$sim_obs)
sim_dir_alt = system.file("extdata", "PosteriorPredictiveChecks","simulated_observations_no_param_var", package = "r4Casal2", mustWork = TRUE)

sim_vals_alt = read.simulated.data(dir = sim_dir_alt, mean_age = F)
# check no trouble with files
sim_vals_alt$failed_files
# 
names(sim_vals_alt$sim_obs)
```
## Posterior predictive checks
Once simulated data has been read into the R environment, we want to compare where the observed values fall relative to the posterior predictive distributions. We recommend using the DHarma r package for this. To intepret P-values or understand the test-statistics that DHARMa does copy this into your R console `vignette("DHARMa", package="DHARMa")` (Assuming you have installed this package).

```{r ppp, fig.width=8, fig.height=4}
## Create DHARMa objects and P-values
DHARMaResbio = createDHARMa(simulatedResponse = sim_vals$sim_obs$sim_chatTANbiomass, 
  observedResponse = mpd$chatTANbiomass$Values$observed, 
  fittedPredictedResponse = mpd$chatTANbiomass$Values$expected, integerResponse = F)
## Create DHARMa objects and P-values
## for AF 
mpd$chatTANage$Values$numbers_at_age = mpd$chatTANage$Values$observed * mpd$chatTANage$Values$error_value
year = 1999
obs = mpd$chatTANage$Values$numbers_at_age[mpd$chatTANage$Values$year == year]
DHARMaResAF = createDHARMa(simulatedResponse = sim_vals$sim_obs$sim_chatTANage[[as.character(year)]], observedResponse = obs, 
  fittedPredictedResponse = NULL, integerResponse = F)

plot(DHARMaResbio, quantreg = F)
plot(DHARMaResAF, quantreg = F)
```
Look at the simulated data without parameter uncertainty

```{r ppp_no_param_var, fig.width=8, fig.height=4}
## Create DHARMa objects and P-values
DHARMaResbio = createDHARMa(simulatedResponse = sim_vals_alt$sim_obs$sim_chatTANbiomass, 
  observedResponse = mpd$chatTANbiomass$Values$observed, 
  fittedPredictedResponse = mpd$chatTANbiomass$Values$expected, integerResponse = F)
## Create DHARMa objects and P-values
## for AF 
year = 2000
obs = mpd$chatTANage$Values$numbers_at_age[mpd$chatTANage$Values$year == year]

DHARMaResAF = createDHARMa(simulatedResponse = sim_vals_alt$sim_obs$sim_chatTANage[[as.character(year)]], observedResponse = obs, 
  fittedPredictedResponse = NULL, integerResponse = T)

plot(DHARMaResbio, quantreg = F)
plot(DHARMaResAF, quantreg = F)
```

Some other visualizations plots

```{r ggplots_ppp, fig.width=8, fig.height=6}
########################
## boxplot predictive distribution vs observation
sim_data = sim_vals$sim_obs$sim_chatTANbiomass
rownames(sim_data) = mpd$chatTANbiomass$Values$year

bioplt = plot_predictive_dist(sim_data = sim_data, 
    obs = data.frame(obs = mpd$chatTANbiomass$Values$observed, 
    year = mpd$chatTANbiomass$Values$year), lab = "chatTANbiomass", plot_type = "violin")

sim_data = sim_vals$sim_obs$sim_chatTANage[[as.character(year)]]
rownames(sim_data) = unique(mpd$chatTANage$Values$age)
pppAFplt = plot_predictive_dist(sim_data = sim_data, 
          obs = data.frame(obs = obs, year = unique(mpd$chatTANage$Values$age)),
          lab = "Mean Age chatTANage", plot_type = "violin")
```

```{r compare_EM_OM, fig.width=8, fig.height=6}
OM_file = system.file("extdata", "PosteriorPredictiveChecks","OM","OM_vary.log", package = "r4Casal2", mustWork = TRUE)
OM_run = extract.mpd(file = OM_file)

## plot SSBs
my_plot = r4Casal2::plot.derived_quantities(model = list(OM = OM_run, EM = mpd), report_label = "biomass_t1")
my_plot = my_plot + xlab("SSB") + ylim(0, 120000)
```

```{r compare_selectivities, fig.width=8, fig.height=6}
## plot selectivities
#my_plot = r4Casal2::plot.selectivities(model = list(OM = OM_run, EM = mpd), report_label = "biomass_t1")
#my_plot = my_plot + xlab("SSB") + ylim(0, 120000)
```


## PIT residuals
Pearson residuals for multinomial distributed random variables can be difficult to intepret for a lot of reasons, data-weighting for mean age to standardised residuals = 1 @francis2011data, sparsity can create funny patterns etc. An alternative is to use randomised quantile PIT residuals [@warton2017pit, @dunn1996randomized] 


Assuming we have data denoted by $y$ which has cumulative distribution function $F(y; \theta), u = F(y; \theta) \sim Uniform(0,1)$. For discrete variables the following adjustment can be made
\begin{align}
u_i = q_i F(y; \theta) + (1 - q_i) F(y^{-}; \theta) 
\end{align}
where, $q_i$ is a standard uniform random variable and $y^{-}$ is the previous allowable value for $y$.
they do not behave like residuals in the usual sense they are centred around a value of 0.5 rather than a value of 0, and are bounded between 0 and 1.

```{r illustrate_PIT}
# hello
set.seed(123)
n = 50
nsims = 1000
x = rnorm(n, 5, 3)
beta0 = 3
beta1 = 1.2
true_beta1 = 1
y = rpois(n, beta0 + true_beta1 * x)
y_sim = matrix(nrow = n, ncol = nsims) 
for(i in 1:nsims)
  y_sim[,i] = rpois(n, beta0 + beta1 * x)
## calculate PIT
PITResiduals = rep(NA, n)
altPITResiduals = rep(NA, n)
for (i in 1:n){
  minSim <- mean(y_sim[i,] < y[i]) 
  maxSim <- mean(y_sim[i,] <= y[i]) 
  if (minSim == maxSim) {
    PITResiduals[i] = minSim
  } else {
    PITResiduals[i] = runif(1, minSim, maxSim)
  }
  qi = runif(1)
  lower_limit = mean(y_sim[i,] < y[i])
  cum_ecdf = mean(y_sim[i,] <= y[i])
  altPITResiduals[i] = qi * cum_ecdf + (1 - qi) * lower_limit
}
plot(PITResiduals, altPITResiduals)
# It is assumed that PIT resiudals are from uniform(0,1)
# most people transform them to normal distribution for testing
# for familiarity more than anything.
normal_transformed = qnorm(PITResiduals)
qqnorm(normal_transformed)
qqline(normal_transformed, col = "steelblue", lwd = 3, lty = 2)
shapiro.test(normal_transformed)
# From the output, the p-value > 0.05 implying that the distribution 
# of the data are not significantly different from normal distribution. 
# In other words, we can assume the normality.
```


# References
<div id="refs"></div>
